% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LogLaplace.R
\name{MCMC_LLAP}
\alias{MCMC_LLAP}
\title{MCMC algorithm for the log-Laplace model}
\usage{
MCMC_LLAP(
  N,
  thin,
  burn,
  Time,
  Cens,
  X,
  Q = 1,
  beta0 = NULL,
  sigma20 = NULL,
  prior = 2,
  set = TRUE,
  eps_l = 0.5,
  eps_r = 0.5
)
}
\arguments{
\item{N}{Total number of iterations. Must be a multiple of thin.}

\item{thin}{Thinning period.}

\item{burn}{Burn-in period. Must be a multiple of thin.}

\item{Time}{Vector containing the survival times.}

\item{Cens}{Censoring indication (1: observed, 0: right-censored).}

\item{X}{Design matrix with dimensions \eqn{n} x  \eqn{k} where \eqn{n} is
the number of observations and \eqn{k} is the number of covariates
(including the intercept).}

\item{Q}{Update period for the \eqn{\lambda_{i}}â€™s}

\item{beta0}{Starting values for \eqn{\beta}. If not provided, they will
be randomly generated from a normal distribution.}

\item{sigma20}{Starting value for \eqn{\sigma^2}. If not provided, it
will be randomly generated from a gamma distribution.}

\item{prior}{Indicator of prior (1: Jeffreys, 2: Type I Ind. Jeffreys,
3: Ind. Jeffreys).}

\item{set}{Indicator for the use of set observations (1: set observations,
0: point observations). The former is strongly recommended over the
latter as point observations cause problems in the context of Bayesian
inference (due to continuous sampling models assigning zero probability
to a point).}

\item{eps_l}{Lower imprecision \eqn{(\epsilon_l)} for set observations
(default value: 0.5).}

\item{eps_r}{Upper imprecision \eqn{(\epsilon_r)} for set observations
(default value: 0.5)}
}
\value{
A matrix with \eqn{N / thin + 1} rows. The columns are the MCMC
chains for \eqn{\beta} (\eqn{k} columns), \eqn{\sigma^2} (1 column),
\eqn{\theta} (1 column, if appropriate), \eqn{\lambda} (\eqn{n} columns,
not provided for log-normal model), \eqn{\log(t)} (\eqn{n} columns,
simulated via data augmentation) and the logarithm of the adaptive
variances (the number varies among models). The latter allows the user to
evaluate if the adaptive variances have been stabilized.
}
\description{
Adaptive Metropolis-within-Gibbs algorithm with univariate
Gaussian random walk proposals for the log-Laplace model
}
\examples{
library(BASSLINE)

# Please note: N=1000 is not enough to reach convergence.
# This is only an illustration. Run longer chains for more accurate
# estimations.

LLAP <- MCMC_LLAP(N = 1000, thin = 20, burn = 40, Time = cancer[, 1],
                  Cens = cancer[, 2], X = cancer[, 3:11])

}
